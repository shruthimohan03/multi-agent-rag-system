News
July 24, 2024

We are releasing Stable Video 4D (SV4D), a video-to-4D diffusion model for novel-view video synthesis. For research purposes:
SV4D was trained to generate 40 frames (5 video frames x 8 camera views) at 576x576 resolution, given 5 context frames (the input video), and 8 reference views (synthesised from the first frame of the input video, using a multi-view diffusion model like SV3D) of the same size, ideally white-background images with one object.
To generate longer novel-view videos (21 frames), we propose a novel sampling method using SV4D, by first sampling 5 anchor frames and then densely sampling the remaining frames while maintaining temporal consistency.
To run the community-build gradio demo locally, run python -m scripts.demo.gradio_app_sv4d.
Please check our project page, tech report and video summary for more details.
QUICKSTART : python scripts/sampling/simple_video_sample_4d.py --input_path assets/sv4d_videos/test_video1.mp4 --output_folder outputs/sv4d (after downloading sv4d.safetensors and sv3d_u.safetensors from HuggingFace into checkpoints/)

To run SV4D on a single input video of 21 frames:

Download SV3D models (sv3d_u.safetensors and sv3d_p.safetensors) from here and SV4D model (sv4d.safetensors) from here to checkpoints/

Run python scripts/sampling/simple_video_sample_4d.py --input_path <path/to/video>

input_path : The input video <path/to/video> can be
a single video file in gif or mp4 format, such as assets/sv4d_videos/test_video1.mp4, or
a folder containing images of video frames in .jpg, .jpeg, or .png format, or
a file name pattern matching images of video frames.
num_steps : default is 20, can increase to 50 for better quality but longer sampling time.
sv3d_version : To specify the SV3D model to generate reference multi-views, set --sv3d_version=sv3d_u for SV3D_u or --sv3d_version=sv3d_p for SV3D_p.
elevations_deg : To generate novel-view videos at a specified elevation (default elevation is 10) using SV3D_p (default is SV3D_u), run python scripts/sampling/simple_video_sample_4d.py --input_path assets/sv4d_videos/test_video1.mp4 --sv3d_version sv3d_p --elevations_deg 30.0
Background removal : For input videos with plain background, (optionally) use rembg to remove background and crop video frames by setting --remove_bg=True. To obtain higher quality outputs on real-world input videos with noisy background, try segmenting the foreground object using Clipdrop or SAM2 before running SV4D.
Low VRAM environment : To run on GPUs with low VRAM, try setting --encoding_t=1 (of frames encoded at a time) and --decoding_t=1 (of frames decoded at a time) or lower video resolution like --img_size=512.
tile

March 18, 2024

We are releasing SV3D, an image-to-video model for novel multi-view synthesis, for research purposes:
SV3D was trained to generate 21 frames at resolution 576x576, given 1 context frame of the same size, ideally a white-background image with one object.
SV3D_u: This variant generates orbital videos based on single image inputs without camera conditioning..
SV3D_p: Extending the capability of SVD3_u, this variant accommodates both single images and orbital views allowing for the creation of 3D video along specified camera paths.
We extend the streamlit demo scripts/demo/video_sampling.py and the standalone python script scripts/sampling/simple_video_sample.py for inference of both models.
Please check our project page, tech report and video summary for more details.
To run SV3D_u on a single image:

Download sv3d_u.safetensors from https://huggingface.co/stabilityai/sv3d to checkpoints/sv3d_u.safetensors
Run python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_u
To run SV3D_p on a single image:

Download sv3d_p.safetensors from https://huggingface.co/stabilityai/sv3d to checkpoints/sv3d_p.safetensors
Generate static orbit at a specified elevation eg. 10.0 : python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_p --elevations_deg 10.0
Generate dynamic orbit at a specified elevations and azimuths: specify sequences of 21 elevations (in degrees) to elevations_deg ([-90, 90]), and 21 azimuths (in degrees) to azimuths_deg [0, 360] in sorted order from 0 to 360. For example: python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_p --elevations_deg [<list of 21 elevations in degrees>] --azimuths_deg [<list of 21 azimuths in degrees>]
To run SVD or SV3D on a streamlit server: streamlit run scripts/demo/video_sampling.py

tile

November 30, 2023

Following the launch of SDXL-Turbo, we are releasing SD-Turbo.
November 28, 2023

We are releasing SDXL-Turbo, a lightning fast text-to image model. Alongside the model, we release a technical report

Usage:
Follow the installation instructions or update the existing environment with pip install streamlit-keyup.
Download the weights and place them in the checkpoints/ directory.
Run streamlit run scripts/demo/turbo.py.
tile

November 21, 2023

We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:

SVD: This model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size. We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware deflickering decoder.
SVD-XT: Same architecture as SVD but finetuned for 25 frame generation.
You can run the community-build gradio demo locally by running python -m scripts.demo.gradio_app.
We provide a streamlit demo scripts/demo/video_sampling.py and a standalone python script scripts/sampling/simple_video_sample.py for inference of both models.
Alongside the model, we release a technical report.
tile

July 26, 2023

We are releasing two new open models with a permissive CreativeML Open RAIL++-M license (see Inference for file hashes):
SDXL-base-1.0: An improved version over SDXL-base-0.9.
SDXL-refiner-1.0: An improved version over SDXL-refiner-0.9.
sample2

July 4, 2023

A technical report on SDXL is now available here.
June 22, 2023

We are releasing two new diffusion models for research purposes:
SDXL-base-0.9: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The base model uses OpenCLIP-ViT/G and CLIP-ViT/L for text encoding whereas the refiner model only uses the OpenCLIP model.
SDXL-refiner-0.9: The refiner has been trained to denoise small noise levels of high quality data and as such is not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.
If you would like to access these models for your research, please apply using one of the following links: SDXL-0.9-Base model, and SDXL-0.9-Refiner. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your Hugging Face Account with your organization email to request access. We plan to do a full release soon (July).

The codebase
General Philosophy
Modularity is king. This repo implements a config-driven approach where we build and combine submodules by calling instantiate_from_config() on objects defined in yaml configs. See configs/ for many examples.

Changelog from the old ldm codebase
For training, we use PyTorch Lightning, but it should be easy to use other training wrappers around the base modules. The core diffusion model class (formerly LatentDiffusion, now DiffusionEngine) has been cleaned up:

No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial conditionings, and all combinations thereof) in a single class: GeneralConditioner, see sgm/modules/encoders/modules.py.
We separate guiders (such as classifier-free guidance, see sgm/modules/diffusionmodules/guiders.py) from the samplers (sgm/modules/diffusionmodules/sampling.py), and the samplers are independent of the model.
We adopt the "denoiser framework" for both training and inference (most notable change is probably now the option to train continuous time models):
Discrete times models (denoisers) are simply a special case of continuous time models (denoisers); see sgm/modules/diffusionmodules/denoiser.py.
The following features are now independent: weighting of the diffusion loss function (sgm/modules/diffusionmodules/denoiser_weighting.py), preconditioning of the network (sgm/modules/diffusionmodules/denoiser_scaling.py), and sampling of noise levels during training (sgm/modules/diffusionmodules/sigma_sampling.py).
Autoencoding models have also been cleaned up.